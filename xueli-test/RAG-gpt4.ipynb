{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an assistant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "\n",
    "\n",
    "assistant_name = 'sem-tab'\n",
    "assistant_instruction = '....'\n",
    "path = 'path'\n",
    "vector_store_name = 'sem-tab-input'\n",
    "key = os.environ.get(\"OPENAI_API_KEY\")\n",
    "temp = 0.2\n",
    "\n",
    "client = OpenAI(api_key=key)\n",
    "assistant = client.beta.assistants.create(\n",
    "    name = assistant_name,\n",
    "    instructions=assistant_instruction,\n",
    "    tools=[{\"type\": \"file_search\"}],\n",
    "    model=\"gpt-4-turbo\",\n",
    "    temperature=temp,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload files and add them to a Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed\n",
      "FileCounts(cancelled=0, completed=15, failed=0, in_progress=0, total=15)\n"
     ]
    }
   ],
   "source": [
    "def get_file_paths(folder_path):\n",
    "    file_paths = []\n",
    "    # Iterate through all files in the folder\n",
    "    for root, dirs, files in os.walk(folder_path):\n",
    "        for file_name in files:\n",
    "            # Get the absolute path of the file\n",
    "            file_path = os.path.join(root, file_name)\n",
    "            # Append the file path to the list\n",
    "            file_paths.append(file_path)\n",
    "    return file_paths\n",
    "\n",
    "# create a Vector Store\n",
    "vector_store = client.beta.vector_stores.create(name=vector_store_name)\n",
    "\n",
    "folder = path\n",
    "file_paths = get_file_paths(folder)\n",
    "file_streams = [open(path, 'rb') for path in file_paths]\n",
    "file_batch = client.beta.vector_stores.file_batches.upload_and_poll(\n",
    "    vector_store_id=vector_store.id,\n",
    "    files=file_streams\n",
    ")\n",
    "\n",
    "print(file_batch.status)\n",
    "print(file_batch.file_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update the assistant to use the new Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "assistant = client.beta.assistants.update(\n",
    "    assistant_id=assistant.id,\n",
    "    tools=[{\"type\": \"file_search\"}],\n",
    "    tool_resources={\"file_search\": {\"vector_store_ids\": [vector_store.id]}},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a thread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "thread = client.beta.threads.create()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add a message to the thread"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### define get responce function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response(query,client,assistant,thread):\n",
    "    # add message to the thread\n",
    "    message = client.beta.threads.messages.create(\n",
    "        thread_id=thread.id,\n",
    "        role=\"user\",\n",
    "        content=query\n",
    "    )\n",
    "\n",
    "    # create a run\n",
    "    run = client.beta.threads.runs.create_and_poll(\n",
    "        thread_id=thread.id,\n",
    "        assistant_id=assistant.id\n",
    "    )\n",
    "\n",
    "    # get messages\n",
    "    messages = list(client.beta.threads.messages.list(thread_id=thread.id, run_id=run.id))\n",
    "    message_content = messages[0].content[0].text\n",
    "    # print(f'message_content: {message_content}')\n",
    "    annotations = message_content.annotations\n",
    "    citations = []\n",
    "    for index, annotation in enumerate(annotations):\n",
    "        message_content.value = message_content.value.replace(annotation.text, f\"[{index}]\")\n",
    "        if file_citation := getattr(annotation, \"file_citation\", None):\n",
    "            cited_file = client.files.retrieve(file_citation.file_id)\n",
    "            citations.append(f\"[{index}] {cited_file.filename}\")\n",
    "\n",
    "    # print(message_content.value)\n",
    "    # print(\"\\n\".join(citations))\n",
    "    return message_content.value,citations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### query 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************\n",
      "query: what is requirments engineering\n",
      "response: Requirements Engineering (RE) is a discipline within software engineering and systems engineering that focuses on determining the needs or conditions to meet for a new or altered product, taking account of the possibly conflicting requirements of the various stakeholders, such as beneficiaries or users. RE involves various activities such as requirements elicitation, requirements analysis, requirements specification, requirements validation, and requirements management. The goal is to produce a comprehensive and detailed set of requirements for the system that can serve as a basis for subsequent stages of product development, ensuring that the final product meets the needs of its users and stakeholders.\n",
      "reference: []\n"
     ]
    }
   ],
   "source": [
    "query = \"what is requirments engineering\"\n",
    "response = get_response(query,client,assistant,thread)\n",
    "print(\"************\")\n",
    "print(f\"query: {query}\")\n",
    "print(f'response: {response[0]}')\n",
    "print(f\"reference: {response[1]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
